{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep RL Home Work 1\n",
    "\n",
    "Author: Kay Ke (kayke@uw.edu)\n",
    "Update: Sep 12, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The typical imports\n",
    "%matplotlib inline\n",
    "\n",
    "# DotDict\n",
    "class DotDict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "args = DotDict({\n",
    "    'envname':'Hopper-v1',\n",
    "    'expert_policy_file' : 'experts/Hopper-v1.pkl',\n",
    "    'num_rollouts' : 10,\n",
    "    'render' : True,\n",
    "    'max_timesteps' : 400,\n",
    "    'verbose' : 0,\n",
    "    'dagger' : True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Code to load an expert policy and generate roll-out data for behavioral cloning.\n",
    "Example usage:\n",
    "    python run_expert.py experts/Humanoid-v1.pkl Humanoid-v1 --render \\\n",
    "            --num_rollouts 20\n",
    "\n",
    "Author of this script and included expert policies: Jonathan Ho (hoj@openai.com)\n",
    "\n",
    "- The structures (functions, class design) are borrowed from\n",
    "    https://github.com/EbTech/CS294/blob/master/hw1/run_expert.py\n",
    "    just to get familiar with TF + Keras\n",
    "- Kay, 2017 Sep 11.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import load_policy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import tf_util\n",
    "\n",
    "# Suppress instruction set warning on mac\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "# Immitation Learning: learn a mapping from observations to actions.\n",
    "class Agent:\n",
    "    def __init__(self, dim):\n",
    "        self.model = Sequential();\n",
    "        self.model.add(Dense(64, input_dim=dim[0], activation='relu'))\n",
    "        self.model.add(Dense(units=dim[1]))\n",
    "        self.model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "    def train(self, training_data, batch_size, epochs, verbose):\n",
    "        print(\"Train the agent with %i training data, batch_size %i, epochs %i\" % (training_data[0].shape[0], batch_size, epochs))\n",
    "        self.history = self.model.fit(training_data[0], training_data[1],\n",
    "                    batch_size, epochs, verbose)\n",
    "        print(\"Training loss at last epoch %f\" % np.mean(self.history.history['loss'][-1]))\n",
    "\n",
    "    def save_training_history(self):\n",
    "        print(self.history.history.keys())\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        fig, ax = plt.subplots( nrows=1, ncols=2 )\n",
    "        ax.plot(self.history.history['loss'])\n",
    "        #ax.plot(self.history.history['val_loss'])\n",
    "        ax.set_title('model loss')\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.set_xlabel('epoch')\n",
    "        #ax.legend(['train', 'test'], loc='upper left')\n",
    "        fig.savefig('training_report.png')   # save the figure to file\n",
    "        plt.close(fig)    # close the figure\n",
    "\n",
    "    def act(self, obs):\n",
    "        X = np.expand_dims(obs, 0)\n",
    "        return self.model.predict(X, batch_size=200, verbose=0)\n",
    "\n",
    "# Simple wrapper around policy function to have an act function\n",
    "class Expert:\n",
    "    def __init__(self, expert_policy_file):\n",
    "        print('loading and building expert policy')\n",
    "        self.policy_fn = load_policy.load_policy(expert_policy_file)\n",
    "        print('loaded and built')\n",
    "\n",
    "    def act(self, obs):\n",
    "        return self.policy_fn(obs[None,:])\n",
    "    \n",
    "class Stimulator:\n",
    "    def __init__(self, envname):\n",
    "        self.init(envname)\n",
    "        self.envname = envname\n",
    "        \n",
    "    def init(self, envname):\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        self.env = gym.make(envname)\n",
    "    \n",
    "    def stimulate(self, agent, max_steps, num_rollouts, render, verbose):\n",
    "        with self.session.as_default():\n",
    "            returns = []\n",
    "            observations = []\n",
    "            actions = []\n",
    "            for i in range(num_rollouts):\n",
    "                if verbose > 0:\n",
    "                    print('iter', i)\n",
    "                obs = self.env.reset()\n",
    "                done = False\n",
    "                totalr = 0.\n",
    "                steps = 0\n",
    "                while not done:\n",
    "                    action = agent.act(obs)\n",
    "                    observations.append(obs)\n",
    "                    actions.append(np.squeeze(action))\n",
    "                    obs, r, done, _ = self.env.step(action)\n",
    "                    totalr += r\n",
    "                    steps += 1\n",
    "                    if render:\n",
    "                        self.env.render()\n",
    "                    if steps % 100 == 0 and verbose >=2:\n",
    "                        print(\"%i/%i\"%(steps, max_steps))\n",
    "                    if steps >= max_steps:\n",
    "                        break\n",
    "                if verbose >= 3 and steps < max_steps:\n",
    "                    print('Died prematurely at step %i' % steps)\n",
    "                if verbose >= 1:\n",
    "                    print('rollout %i/%i return=%f' % (i+1, num_rollouts, totalr))\n",
    "                returns.append(totalr)\n",
    "            if verbose > 0:\n",
    "                print('Return summary: mean=%f, std=%f' % (np.mean(returns), np.std(returns)))\n",
    "\n",
    "            return (np.array(observations), np.array(actions))\n",
    "    \n",
    "    def label_obs(self, expert, obs):\n",
    "        with self.session.as_default():\n",
    "            return expert.policy_fn(stimulated_env)\n",
    "    \n",
    "    def close(self):\n",
    "        self.session.close()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.close()\n",
    "        self.init(self.envname)\n",
    "\n",
    "# Parsing argument to main\n",
    "def parse_argument():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('expert_policy_file', type=str)\n",
    "    parser.add_argument('envname', type=str)\n",
    "    parser.add_argument('--render', action='store_true')\n",
    "    parser.add_argument(\"--max_timesteps\", type=int, default=500)\n",
    "    parser.add_argument('--num_rollouts', type=int, default=1,\n",
    "                        help='Number of expert roll outs')\n",
    "    parser.add_argument('--dagger', action='store_true') \n",
    "\n",
    "    # Verbose Level\n",
    "    parser.add_argument('-v', '--verbose', type=int, choices=[0, 1, 2], default=1)\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building expert policy\n",
      "obs (1, 11) (1, 11)\n",
      "loaded and built\n"
     ]
    }
   ],
   "source": [
    "# Load Expert\n",
    "expert = Expert(args.expert_policy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-13 01:28:41,600] Making new env: Hopper-v1\n"
     ]
    }
   ],
   "source": [
    "# Generate training data\n",
    "stimulator = Stimulator(args.envname)\n",
    "training_data = stimulator.stimulate(expert, max_steps=300, num_rollouts=20, render=False, verbose=0)\n",
    "dim = (training_data[0].shape[-1], training_data[1].shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the agent with 6000 training data, batch_size 512, epochs 80\n",
      "Training loss at last epoch 0.131083\n",
      "Train the agent with 6000 training data, batch_size 512, epochs 80\n",
      "Training loss at last epoch 0.126318\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(dim)\n",
    "agent.train(training_data, batch_size=512, epochs=80, verbose=0)\n",
    "agent_dagger = Agent(dim)\n",
    "agent_dagger.train(training_data, batch_size=512, epochs=80, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-13 01:30:06,591] Making new env: Hopper-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "Train the agent with 6000 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.036458\n",
      "iter 1\n",
      "Train the agent with 6000 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.033599\n",
      "iter 2\n",
      "Train the agent with 6000 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.033191\n",
      "iter 3\n",
      "Train the agent with 6000 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.031302\n",
      "iter 4\n",
      "Train the agent with 6000 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.030127\n",
      "iter 5\n",
      "Train the agent with 6000 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.029688\n",
      "iter 6\n",
      "Train the agent with 6000 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.027383\n",
      "iter 7\n",
      "Train the agent with 6000 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.026560\n",
      "iter 8\n",
      "Train the agent with 6000 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.025161\n",
      "iter 9\n",
      "Train the agent with 6000 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.026545\n"
     ]
    }
   ],
   "source": [
    "stimulator.reset()\n",
    "for i in range(0, 10):\n",
    "    # Behavior Cloning\n",
    "    print(\"iter %i\" % i)\n",
    "    training_data = stimulator.stimulate(expert, max_steps=args.max_timesteps, num_rollouts=args.num_rollouts, render=False, verbose=0)\n",
    "    agent.train(training_data, batch_size=512, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-13 01:31:00,903] Making new env: Hopper-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.077217\n",
      "iter 1\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.021364\n",
      "iter 2\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.013072\n",
      "iter 3\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.009990\n",
      "iter 4\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.009654\n",
      "iter 5\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.008960\n",
      "iter 6\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.008367\n",
      "iter 7\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.008445\n",
      "iter 8\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.006585\n",
      "iter 9\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.007892\n",
      "iter 10\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.006795\n",
      "iter 11\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.006028\n",
      "iter 12\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.005980\n",
      "iter 13\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.005413\n",
      "iter 14\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.005131\n",
      "iter 15\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.005131\n",
      "iter 16\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.004804\n",
      "iter 17\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.005840\n",
      "iter 18\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.005789\n",
      "iter 19\n",
      "Train the agent with 240 training data, batch_size 512, epochs 50\n",
      "Training loss at last epoch 0.004807\n"
     ]
    }
   ],
   "source": [
    "stimulator.reset()\n",
    "for i in range(0, 20):\n",
    "    # DAgger\n",
    "    print(\"iter %i\" % i)\n",
    "    (stimulated_env, _) = stimulator.stimulate(agent_dagger, max_steps=args.max_timesteps, num_rollouts=args.num_rollouts, render=False, verbose=0)\n",
    "    labels = stimulator.label_obs(expert, stimulated_env)\n",
    "    agent_dagger.train((stimulated_env, labels), batch_size=512, epochs=50, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "rollout 1/20 return=7.539343\n",
      "iter 1\n",
      "rollout 2/20 return=9.260624\n",
      "iter 2\n",
      "rollout 3/20 return=7.927104\n",
      "iter 3\n",
      "rollout 4/20 return=9.921639\n",
      "iter 4\n",
      "rollout 5/20 return=7.869126\n",
      "iter 5\n",
      "rollout 6/20 return=10.394251\n",
      "iter 6\n",
      "rollout 7/20 return=9.751810\n",
      "iter 7\n",
      "rollout 8/20 return=8.731466\n",
      "iter 8\n",
      "rollout 9/20 return=8.789494\n",
      "iter 9\n",
      "rollout 10/20 return=7.467604\n",
      "iter 10\n",
      "rollout 11/20 return=8.146308\n",
      "iter 11\n",
      "rollout 12/20 return=7.500677\n",
      "iter 12\n",
      "rollout 13/20 return=9.004497\n",
      "iter 13\n",
      "rollout 14/20 return=8.860869\n",
      "iter 14\n",
      "rollout 15/20 return=8.144460\n",
      "iter 15\n",
      "rollout 16/20 return=9.336771\n",
      "iter 16\n",
      "rollout 17/20 return=7.071375\n",
      "iter 17\n",
      "rollout 18/20 return=8.285126\n",
      "iter 18\n",
      "rollout 19/20 return=8.519834\n",
      "iter 19\n",
      "rollout 20/20 return=8.030722\n",
      "Return summary: mean=8.527655, std=0.872773\n",
      "iter 0\n",
      "rollout 1/20 return=6.994500\n",
      "iter 1\n",
      "rollout 2/20 return=6.896234\n",
      "iter 2\n",
      "rollout 3/20 return=7.242112\n",
      "iter 3\n",
      "rollout 4/20 return=7.006553\n",
      "iter 4\n",
      "rollout 5/20 return=7.208704\n",
      "iter 5\n",
      "rollout 6/20 return=7.084041\n",
      "iter 6\n",
      "rollout 7/20 return=7.144636\n",
      "iter 7\n",
      "rollout 8/20 return=7.229266\n",
      "iter 8\n",
      "rollout 9/20 return=7.192471\n",
      "iter 9\n",
      "rollout 10/20 return=7.084259\n",
      "iter 10\n",
      "rollout 11/20 return=7.108994\n",
      "iter 11\n",
      "rollout 12/20 return=7.246956\n",
      "iter 12\n",
      "rollout 13/20 return=7.042159\n",
      "iter 13\n",
      "rollout 14/20 return=7.222481\n",
      "iter 14\n",
      "rollout 15/20 return=7.376308\n",
      "iter 15\n",
      "rollout 16/20 return=7.171927\n",
      "iter 16\n",
      "rollout 17/20 return=7.233714\n",
      "iter 17\n",
      "rollout 18/20 return=7.258015\n",
      "iter 18\n",
      "rollout 19/20 return=7.113924\n",
      "iter 19\n",
      "rollout 20/20 return=6.999219\n",
      "Return summary: mean=7.142824, std=0.113846\n"
     ]
    }
   ],
   "source": [
    "_ = stimulator.stimulate(agent, args.max_timesteps, args.num_rollouts, args.render, verbose=1)\n",
    "_ = stimulator.stimulate(agent_dagger, args.max_timesteps, args.num_rollouts, args.render, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
